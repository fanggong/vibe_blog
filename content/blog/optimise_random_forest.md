---
title: 随机森林模型优化
author: Fang Yongchao
date: "2021-04-26"
description: "随机森林是一种基于多棵决策树的集成学习方法，常用于分类与回归任务。本文围绕其主要超参数，概述在构建与调优随机森林模型时的关键考量。"
categories:
  - "机器学习"
---

原文地址：[Random Forest: Hyperparameters and how to fine-tune them](https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d)

参考链接：[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)

随机森林模型的可调参数主要包括：

- 森林中决策树的数量
- 在节点进行分割的判断标准
- 每棵树的最大深度
- 在节点进行分割所需要的最小样本数
- 叶子节点的最大数量
- 寻找最佳分割时使用的特征数量
- 使用bootstrap时的抽样数量

## 森林中决策树的数量

-----------------------------

通过构建一个包含大量决策树的随机森林，可以得到相对低方差的、鲁棒性好的模型，虽然这个模型的构建可能需要花费大量的时间。这里的诀窍在于对数据进行评估：我们有多少的样本，每个样本又有多少的特征可用。

由于随机森林模型的随机性，如果我们的数据中包含大量的特征，那么当决策树过少时会出现一些具有高信息量的特征没有被使用，或者很少被使用的情况。

同样的，对于样本来说，如果数据中包含大量的样本，那么过少的决策树也会导致一些样本没有被使用的情况出现。

随机森林很少会出现过拟合，所以可以通过增加决策树的数量来减少模型的误差，虽然这样会大大增加模型的训练时间。唯一需要注意的是，不需要无脑增加决策树的数量，因为很显然随着决策树数量的增加，单位训练时间提供的误差减少的收益已经不是很高了。

**结论：对森林中的决策树的数量进行调优是没有多大意义的，只需要将它设定为一个较大的且计算上可行的数字就可以了。**

## 在节点进行分割的判断标准

-----------------------------

决策树通过计算判断每个节点上哪个特征的哪个值能最好地分割这个节点。这个判断的标准，在分类模型中会使用Gini或Entropy，而在回归模型中会使用MAE或MSE。

在回归模型中，如果我们的数据中没有包含大量的离群值的话，一般选择使用MSE，因为MSE会对离群值有更大的惩罚。

在分类模型中，Gini相对Entropy来说计算成本更低，但是很难断言Gini和Entropy哪个更好。

**结论：由于不同的判断标准可能会构建完全不同的模型，而且其可选项也只有两种，所以建议两种都试一试吧。**

## 每棵树的最大深度

----------------------------

增加决策树的深度可以增加决策树中包含的特征的信息量。如果是在决策树模型下，这会导致过拟合，但是在随机森林模型中，由于集成的关系，我们可以加大树的深度。

这个参数应该根据特征的数量选择一个合理的数字，然后进行微调。但是在合理区间的微调对于模型的效果的影响微乎其微，所以其实我们在Grid Search的时候可以考虑不对这个参数进行调整。

**结论：对每棵树的最大深度进行调优的意义不大，选择一个合理的数字即可。**

## 在节点进行分割所需要的最小样本数

----------------------------

在参考的原文中没有对这个参数进行太多描述，个人理解调整在节点进行分割所需要的最小样本数也只是一种调整单个决策树形态的手段而已，当这个参数很小时同样会导致单个决策树的过拟合，但是放到整个随机森林模型中时该问题也就不存在了。

**结论：不需要对在节点进行分割所需要的最小样本数这个参数进行调整。**

## 叶子节点的最大数量

----------------------------

在随机森林模型中这个参数不是很重要，但是在决策树模型中，需要对这个参数进行调整以防止过拟合和提高模型的可解释性。

**结论：不需要对叶子节点的最大数量这个参数进行调整。**

## 寻找最佳分割时使用的特征数量

----------------------------

这是随机森林模型调优时最重要的一个参数，最好的办法是通过Grid Search和Cross Validation得到最优参数，这里应该考虑一下几点：

- 使用较少的特征会降低集成的方差，但是会导致较高的单个决策树的偏差
- 这个值应该根据我们的数据中高信息量高质量的特征数量来决定。如果数据中的特征都是非常干净高质量的，那么这个参数可以设定得相对较小。但是如果我们的数据中包含大量的噪声，那么这个参数应该设定的大一些以提高高信息高质量的特征被纳入的机会
- 增加寻找最佳分割时使用的特征数量可以降低模型的偏差，因为好的特征被使用的机会大大增加了。同时，这也会导致模型的方差增大，当然，模型的训练时间也会大大增加

考虑到以上几点，在使用Grid Search对该参数进行调优时，可以尝试一下几个值：

- **None**：使用全部的特征
- **sqrt**：使用全部特征的数量开根号，即如果有25个特征，那么在寻找最佳分割时使用随机的5个特征
- **0.2**：使用全部特征的20%，当然可以把0.3，0.4，0.5...都试一试。对于回归模型来说，0.33是一个好的开始

**结论：在对每个节点进行分割时，仔细调整要考虑的特征数量是基本的，因此考虑使用Grid Search寻找最佳参数。**

## 使用bootstrap时的抽样数量

----------------------------

由于bootstrap采用的是有放回的抽样，所以即使抽样数量与样本总量一致，在训练每棵树时使用的数据也是不同的。

> 袋外数据（Out Of Bag，即进行有放回的抽样时，有大约三分之一多的数据其实是不会被抽到的，这个数据即袋外数据），可以用来作为随机森林的测试集进行模型评估。有点费解，袋外数据应该是相对于每一个决策树存在的，如何作为最终集成的随机森林的测试集。乍一看感觉是有一些反常识的，不知道是否我的理解上有误，或者说这个理论有严格的证明

**结论：不需要对使用bootstrap时的抽样数量这个参数进行调整。**
